!pip install spacy
!pip install newsapi-python

!python -m spacy download en_core_web_sm

import spacy
import en_core_web_sm
from newsapi import NewsApiClient
import pickle
import pandas as pd
import string
import numpy as np
from collections import Counter 
import nltk
from nltk.tokenize import RegexpTokenizer
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from wordcloud import WordCloud 
from textblob import TextBlob
nltk.download('stopwords')
nltk.download('brown')
nltk.download('punkt')

nlp_eng = en_core_web_sm.load()
newsapi = NewsApiClient(api_key= 'd77ffa8bab1740a3af04a59d0ec585e6')

temp = newsapi.get_everything(q='coronavirus', language='en', from_param='2020-10-30', to='2020-11-02', sort_by='relevancy')

filename = 'articlesCOVID.pckl'
pickle.dump(temp, open(filename, 'wb'))

filename = 'articlesCOVID.pckl'
loaded_model = pickle.load(open(filename, 'rb'))

filepath = '/content/articlesCOVID.pckl'
pickle.dump(loaded_model, open(filepath, 'wb'))

df = pd.DataFrame(temp['articles'])

#df['description']
tokenizer = RegexpTokenizer(r'\w+')
def get_keywords_eng(token):
  result = []
  punctuation = string.punctuation
  stop_words = stopwords.words('english')

  for i in token:
    if (i in stop_words):
      continue
  else:
    result.append(i)
  return result

results = []
for content in df.content.values:
    content = tokenizer.tokenize(content)
    results.append([x[0] for x in Counter(get_keywords_eng(content)).most_common(5)])
df['keywords'] = results

text = str(results)
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
